# -*- coding: utf-8 -*-
"""convolution

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ol6hUz-IA9NU3S9IcgtdLKYNXq7o330G
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/database.zip

!pip install --upgrade keras

import os

os.environ["KERAS_BACKEND"] = "tensorflow"

import tensorflow as tf
import keras
import matplotlib.pyplot as plt

# Set seed for reproducibility.
tf.random.set_seed(42)

import matplotlib.pyplot as plt
import numpy as np
import cv2
import sys
from PIL import ImageFont, ImageDraw, Image
import glob

# Get list of directories
listf = sorted(glob.glob("/content/database/mel_spectrum_all_train/*"))
print("List of folders in all_train:", listf)
print("Number of folders:", len(listf))

# Initialize variables
L = 0
num_classes = min(11, len(listf))  # Ensure num_classes doesn't exceed len(listf)

# Count total images
for i in range(0, num_classes):
    listg = glob.glob(listf[i] + "/*png")
    for m in range(0, len(listg)):
        L = L + 1

# Prepare trainData and labels arrays
s = [48, 48]
trainData = np.full((L, s[0], s[1], 3), 1)
labels = np.full(L, 1)

# Process images
cnt = 0
for i in range(0, num_classes):
    listg = glob.glob(listf[i] + "/*png")
    for m in range(0, len(listg)):
        imgname = listg[m]
        image = cv2.imread(imgname)
        image = cv2.resize(image, (s[1], s[0]))
        trainData[cnt, :, :, :] = image
        labels[cnt] = i
        cnt = cnt + 1

print("Data processing complete. Total images:", L)

import tensorflow as tf
import glob
import numpy as np

# Define data paths
train_dir = "/content/database/mel_spectrum_all_train/"
test_dir = "/content/database/melspectrum_all_test/"

# Define image size and batch size
img_size = (48, 48)
batch_size = 256

# Create label mapping
label_mapping = {
    'cel': 0, 'cla': 1, 'flu': 2, 'gac': 3, 'gel': 4,
    'org': 5, 'pia': 6, 'sax': 7, 'tru': 8, 'vio': 9, 'voice': 10,
    'voi': 10 # Add this mapping
}

# Load training images and labels
train_image_paths = glob.glob(train_dir + "/*/*png")  # Ensures it finds png files in subfolders
train_labels = [label_mapping.get(path.split("/")[-2], -1) for path in train_image_paths] # Changed to use dict.get() for safer key access

# Check if any label is -1 (unknown) and handle it appropriately.
if -1 in train_labels:
    print("WARNING: Unknown labels found. Check your dataset and label mapping.")
    # Here you can decide how to handle unknown labels,
    # for example, by removing the corresponding images:
    # train_image_paths = [path for path, label in zip(train_image_paths, train_labels) if label != -1]
    # train_labels = [label for label in train_labels if label != -1]

# Load training images
def load_image(path):
    img = tf.keras.utils.load_img(path)  # Use keras.utils.load_img to load image
    img = tf.keras.utils.img_to_array(img)  # Convert image to array
    img = tf.image.resize(img, (32,32))
    img = img / 255.0
    return img

# Check if train_image_paths is empty and print its content
if not train_image_paths:
    print("train_image_paths is empty. Check your train_dir:", train_dir)
    print("Files found in train_dir:")
    import os
    for filename in os.listdir(train_dir):
        print(filename)
else:
    train_images = np.array([load_image(path) for path in train_image_paths])
    train_labels = np.array(train_labels, dtype=np.int32)

    # Split dataset into training and validation sets
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=32)

    # Create TensorFlow datasets
    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)
    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)

    print("Training dataset:", train_ds)
    print("Validation dataset:", val_ds)

import matplotlib.pyplot as plt

class_names = ['cel', 'cla', 'flu', 'gac', 'gel', 'org', 'pia', 'sax', 'tru', 'vio', 'voice']

import random

random_indices = random.sample(range(len(train_images)), 25)

plt.figure(figsize=(10, 10))
for i, idx in enumerate(random_indices):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[idx])
    plt.xlabel(class_names[train_labels[idx]])

plt.tight_layout()
plt.show()

print("building the convolution model...")

conv_model = keras.Sequential([
    keras.layers.InputLayer(input_shape=(32,32, 3)),
    keras.layers.Conv2D(256, (3, 3), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2,2)),
    keras.layers.Dropout(0.2),

    keras.layers.Conv2D(256, (3, 3), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPooling2D((2,2)),
    keras.layers.Dropout(0.2),

    keras.layers.Conv2D(256, (3, 3), activation='relu', padding="same"),
    keras.layers.BatchNormalization(),
    keras.layers.Flatten(),

    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(11, activation='softmax')
])

print("compiling the convolution model...")

conv_model.compile(
    optimizer="adam",
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=["accuracy"],
)

print("conv model training...")

conv_hist = conv_model.fit(train_ds, epochs=100, validation_data=val_ds)

import time
import tensorflow as tf
from tensorflow import keras

# Measure training time
start_time = time.time()

conv_hist = conv_model.fit(train_ds, epochs=1, validation_data=val_ds)

end_time = time.time()

print(f"Total Training Time: {end_time - start_time:.2f} seconds")

conv_model.summary()

import matplotlib.pyplot as plt
import numpy as np
import cv2
import sys
from PIL import ImageFont, ImageDraw, Image
import glob

# Get list of directories, ensure the path is correct
listf = sorted(glob.glob("/content/database/melspectrum_all_test/*"))
print(listf)  # Print listf to verify its contents
L = 0
num_classes = 11

# Handle case where listf is empty
if not listf:
    print("Error: No directories found at the specified path.")
    sys.exit(1)  # Exit the script with an error code

# Initialize labels array with the correct size
labels = np.empty(0, dtype=int)  # Initialize as an empty array

# Restrict loop to actual number of folders found
for i in range(0, min(num_classes, len(listf))):
    listg = glob.glob(listf[i] + "/*png")
    L += len(listg)  # Update L with the number of images in each class
    # Append labels for the current class to the labels array
    labels = np.append(labels, np.full(len(listg), i, dtype=int))

s = [32, 32]
testData = np.full((L, s[0], s[1], 3), 1)
cnt = 0
F = []

for i in range(0, min(num_classes, len(listf))):  # Ensure i stays within bounds
    listg = sorted(glob.glob(listf[i] + "/*png"))
    for m in range(0, len(listg)):
        imgname = listg[m]
        image = cv2.imread(imgname)
        image = cv2.resize(image, (s[1], s[0]))
        testData[cnt, :, :, :] = image
        # Labels are already assigned, no need to reassign here
        # labels[cnt] = i
        cnt = cnt + 1

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

test_images = testData.astype('float32') / 255
test_labels = labels  # Use the 'labels' variable instead of 'testlabels'
print(test_images.shape)
print(test_labels.shape)
labels = inv_model.predict(test_images)
test_out = np.argmax(np.round(labels),axis=1)
print(test_labels) # Print test_labels instead of testlabels
print(test_out)
test_loss, test_acc = inv_model.evaluate(test_images, test_labels)
#print("Test loss: {0:.6f}, Test accuracy: {1:.6f}".format(test_loss,test_acc))
print('Test accuracy:', test_acc*100)
cm = confusion_matrix(test_labels, test_out) # Use test_labels instead of testlabels
print(cm)
cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
print(cm.diagonal())
a = inv_model.predict(test_images)
#print(a)
import pandas as pd
pd.DataFrame(a).to_csv("a.csv")
target_names = ['cel', 'cla', 'flu', 'gac', 'gel','org', 'pia','sax','tru','vio','voice']
print(classification_report(test_labels, test_out, target_names=target_names)) # Use test_labels instead of testlabels
# Plot confusion matrix
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()





